{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46f1f006-c5df-4aa4-ba36-9b3f70552484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 1 :\n",
    "# R-squared \n",
    "# also known as the coefficient of determination, is a statistical measure used to evaluate the goodness of fit of a linear regression model.\n",
    "# It is calculated by \n",
    "\n",
    "# **R^2 = 1-SSR/SST**\n",
    "# - R^2 = accuracy\n",
    "# - SSR = sum of squares of residuals\n",
    "# - SST = total sum of squares\n",
    "\n",
    "# It represents the accuracy of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cf63f97-28f5-4942-886b-905b2eb68331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 2 :\n",
    "# Adjusted R squared \n",
    "# Adjusted R-squared is a modified version of the regular R-squared that takes \n",
    "#   into account the number of predictors (independent variables) in a linear regression model\n",
    "# It addresses a limitation of R-squared by penalizing the inclusion of unnecessary variables that may \n",
    "#   artificially inflate the regular R-squared value.\n",
    "\n",
    "# It is diffrent from R squared \n",
    "# Regular R-squared can be overly optimistic and tend to increase with the addition of more predictors, \n",
    "#  regardless of their actual significance or contribution to the model.\n",
    "# where adjusted R square Adjusted R-squared adjusts the R-squared value by incorporating a penalty term \n",
    "#  that accounts for the number of predictors and the sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffa97271-b892-4cc7-be6d-e13463b6c1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 3 :\n",
    "# It is appropriate to use adjusted R square there n number of features but only some features are useful to predict the model\n",
    "# Adjusted R-squared is especially useful when you want to assess the impact of adding or removing predictors on the model's fit. \n",
    "# If you add more predictors to a model, the regular R-squared may increase even if the added predictors are not truly significant.\n",
    "# Adjusted R-squared, on the other hand, will adjust for the number of predictors and may decrease if the added predictors do not improve the model significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df76582b-5616-4983-9b6d-507574fdc77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 4 :\n",
    "# RMSE (Root Mean Square Error):\n",
    "# RMSE is a popular metric that provides an overall measure of the prediction error in the units of the dependent variable. \n",
    "# It is calculated by taking the square root of the average of the squared differences between the predicted values and the actual values. \n",
    "# The formula for RMSE is:\n",
    "# RMSE = sqrt( (1/n) * sum( (predicted - actual)^2 ) )\n",
    "\n",
    "# MSE (Mean Squared Error):\n",
    "# MSE is another widely used metric that measures the average squared difference between the predicted values and the actual values. \n",
    "# It is calculated by averaging the squared differences across all observations. \n",
    "# The formula for MSE is:\n",
    "# MSE = (1/n) * sum( (predicted - actual)^2 )\n",
    "\n",
    "# MAE (Mean Absolute Error):\n",
    "# MAE is a metric that measures the average absolute difference between the predicted values and the actual values. \n",
    "# It is calculated by averaging the absolute differences across all observations. \n",
    "# The formula for MAE is:\n",
    "# MAE = (1/n) * sum( |predicted - actual| )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6ae3fb1-c352-447b-8931-86956c733061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 5 :\n",
    "# MSE (Mean Squared Error):\n",
    "# Advantages\n",
    "# 1. equation is diffrentiable by which we calculate slope\n",
    "# 2. It has only one local minima and globbal minima\n",
    "# Disadvantages\n",
    "# 1.It is not robust to outliers\n",
    "# 2.It is not in the same unit\n",
    "\n",
    "# MAE (Mean Absolute Error):\n",
    "# Advantages\n",
    "# 1. It is robust to outliers\n",
    "# 2. It will be in the same unit\n",
    "# Disadvantages\n",
    "# 1.convergence usually takes more time\n",
    "\n",
    "# RMSE (Root Mean Square Error):\n",
    "# 1. equation is diffrentiable by which we calculate slope\n",
    "# 2. It is in the same unit\n",
    "# Disadvantages\n",
    "# 1.It is not robust to outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84b7d06d-0738-4cdb-95bf-2197ef70c905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 6 :\n",
    "# \"Lasso regularization\" or lasso regression\n",
    "# It is type of regression technique \n",
    "# It is used for feature selection \n",
    "# Lasso regularization, also known as L1 regularization, \n",
    "# is a technique used in machine learning to prevent overfitting and improve the performance and interpretability of a model\n",
    "# Lasso has a tendency to shrink some coefficients to exactly zero. This property makes Lasso regularization useful for feature selection\n",
    "\n",
    "# It differs from ridge regression\n",
    "# lasso uses the absolute values of the coefficients , Ridge uses the squared values of the coefficients .\n",
    "# Ridge regularization tends to shrink the coefficients towards zero but does not set them exactly to zero.\n",
    "\n",
    "# Lasso regularization is more suitable when there is a need for feature selection or when the model's interpretability is crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ac4fb8b-61d3-4f8e-9552-e7d4c2b1f015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 7 :\n",
    "# Regularized linear model such as Ridge regression model helps us to reduce overfitting\n",
    "# It type of regression technique\n",
    "# Regularized linear models help prevent overfitting in machine learning by adding a penalty term to the loss function during the model training process. \n",
    "# This penalty term discourages the model from assigning excessively large coefficients to the features, which can lead to overfitting.\n",
    "# Ridge regularization tends to shrink the coefficients towards zero but does not set them exactly to zero.\n",
    "\n",
    "# for example\n",
    "# The loss function can be written as:\n",
    "# Loss = Σ(yᵢ - ȳ)²\n",
    "# Where yᵢ represents the actual values, and ȳ represents the predicted values.\n",
    "# then we  regularization technique call ridge \n",
    "# where the cost function = Loss = Σ(yᵢ - (β₀ + β₁xᵢ))² + λΣβ²\n",
    "# as lamda increases slope or theta starts shrinking which leads to reduce overfitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ed87fff-96e7-4e08-973a-71281328ad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 8 :\n",
    "# Limitations of Regularized linear model\n",
    "# 1.Linearity assumption: Regularized linear models assume a linear relationship between the predictors and the response variable.\n",
    "# 2.Feature selection: While regularized linear models can perform automatic feature selection, \n",
    "#       the methods they employ may not always select the most relevant features for prediction\n",
    "# 3.Sensitivity to outliers: Regularized linear models, especially Ridge regression, are sensitive to outliers in the data. \n",
    "#   Outliers can have a significant impact on the coefficient estimates, leading to biased predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b93be775-cbc6-4456-83a7-8e65ca101db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 9 :\n",
    "# we can't judge a model on the basis  these metrics\n",
    "# RMSE of 10 indicates that, on average, the predictions of Model A have an error of 10 units in the dependent variable. \n",
    "# Meanwhile, MAE of 8 suggests that the average absolute difference between the predicted and actual values of Model B is 8 units. \n",
    "# In terms of magnitude, Model B has a lower error compared to Model A.\n",
    "\n",
    "# It is important to note that choosing the better model based solely on a single metric can be limiting. \n",
    "# It is advisable to consider multiple evaluation metrics, domain knowledge, and the specific goals of the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e9273ba-3aca-4a54-a565-2eb428622caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 10 :\n",
    "# Model selection is done on the basis of specific goals\n",
    "# 1.Prediction accuracy: If the primary goal is to achieve the best prediction accuracy, Model A (Ridge regularization) might be preferred.\n",
    "# 2.Feature selection: If interpretability or reducing the number of predictors is a priority, Model B (Lasso regularization) may be a better choice\n",
    "\n",
    "# limitations\n",
    "# 1.Interpretability: While Lasso regularization provides feature selection and sparsity in the coefficient estimates, \n",
    "#   the interpretability of the remaining non-zero coefficients can be challenging when predictors are highly correlated.\n",
    "# 2.Bias-variance trade-off: Ridge regularization generally introduces less bias but has higher variance compared to Lasso regularization. \n",
    "# 3.Handling multicollinearity:Lasso regularization may arbitrarily select one predictor over another in the presence of high correlation, potentially overlooking important predictors.\n",
    "\n",
    "# coclusion model selection is done on the basis of a specific goal which is appropiate for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5824f547-1d4b-46d0-8084-583ca2271700",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
